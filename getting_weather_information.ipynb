{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rainfall_amount information of 서울특별시 강남구 역삼동 is none.\n",
      "rainfall_amount information of 서울특별시 서초구 방배동 is none.\n",
      "{'서울특별시 강남구 역삼동': {'current_temp': '8', 'min_temp': '0', 'max_temp': '11', 'sensible_temp': '7.5', 'weather': '흐림', 'rainfall_amount': None, 'p_rainfall': '30', 'wind_speed': '1', 'humidity': '53', 'uv_num': '3', 'uv_non_num_content': '보통', 'fine_dust': '좋음', 'fine_dust_num': '11', 'ultra_fine_dust': '좋음', 'ultra_fine_dust_num': '7', 'ozone': '좋음', 'ozone_num': '0.019'}, '서울특별시 서초구 방배동': {'current_temp': '8', 'min_temp': '0', 'max_temp': '11', 'sensible_temp': '7.5', 'weather': '흐림', 'rainfall_amount': None, 'p_rainfall': '30', 'wind_speed': '1', 'humidity': '53', 'uv_num': '3', 'uv_non_num_content': '보통', 'fine_dust': '좋음', 'fine_dust_num': '11', 'ultra_fine_dust': '좋음', 'ultra_fine_dust_num': '7', 'ozone': '좋음', 'ozone_num': '0.019'}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import sys\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "\n",
    "class WeatherInformationCralwer:\n",
    "    def __init__(self):\n",
    "        self._url_dict = {}\n",
    "        self._soup_inst_dict = {}\n",
    "        self._data_dict = {}\n",
    "        \n",
    "        \n",
    "    @property\n",
    "    def data_dict(self):\n",
    "        if len(self._data_dict) == 0:\n",
    "            print(\"Crawled value dictionary is empty. Please check.\")\n",
    "            sys.exit(0)\n",
    "        return self._data_dict \n",
    "    \n",
    "        \n",
    "    def _set_url_dict(self, url_info):\n",
    "        for url in url_info:\n",
    "            key = url.strip().split(\"@\")[0]\n",
    "            value = url.strip().split(\"@\")[1]\n",
    "            if len(value) != 0:\n",
    "                self._url_dict[key] = value\n",
    "                \n",
    "                \n",
    "    def _set__soup_inst_dict(self):\n",
    "        self._soup_inst_dict[\"current_temp\"] = \"global current_temp; current_temp = None; \" \\\n",
    "                                               \"current_temp = soup.find('span', {'class': 'todaytemp'}).text\"\n",
    "        self._soup_inst_dict[\"min_temp\"] = \"global min_temp; min_temp = None; \" \\\n",
    "                                           \"min_temp = soup.find('span', {'class': 'min'}).text[:-1]\"\n",
    "        self._soup_inst_dict[\"max_temp\"] = \"global max_temp; max_temp = None; \" \\\n",
    "                                           \"max_temp = soup.find('span', {'class': 'max'}).text[:-1]\"\n",
    "        self._soup_inst_dict[\"sensible_temp\"] = \"global sensible_temp; sensible_temp = None; \" \\\n",
    "                                                \"sensible_temp = soup.find('span', {'class': 'sensible'})\" \\\n",
    "                                                \".find('em').text[:-1]\"\n",
    "        self._soup_inst_dict[\"weather\"] = \"global weather; weather = None; \" \\\n",
    "                                          \"weather = soup.find('p', {'class': 'cast_txt'}).text.split(',')[0]\"\n",
    "        self._soup_inst_dict[\"rainfall_amount\"] = \"global rainfall_amount; rainfall_amount = None; \" \\\n",
    "                                   \"rainfall_amount = soup.find('span', \" \\\n",
    "                                   \"{'class': 'rainfall'}).text.split()[2].split('mm')[0]\" \n",
    "        self._soup_inst_dict[\"p_rainfall\"] = \"global p_rainfall; p_rainfall = None; \" \\\n",
    "                                   \"p_rainfall = soup.find('div', \" \\\n",
    "                                   \"{'class': 'info_list rainfall _tabContent'}).text.strip().split()[2][:-1]\" \n",
    "        self._soup_inst_dict[\"wind_speed\"] = \"global wind_speed; wind_speed = None; \" \\\n",
    "                                   \"wind_speed = soup.find('div', \" \\\n",
    "                                   \"{'class': 'info_list wind _tabContent'}).text.strip().split()[2][:-3]\"\n",
    "        self._soup_inst_dict[\"humidity\"] = \"global humidity; humidity = None; \" \\\n",
    "                                   \"humidity = soup.find('div', \" \\\n",
    "                                   \"{'class': 'info_list humidity _tabContent'}).text.strip().split()[2][:-1]\"\n",
    "        \n",
    "        \n",
    "    def main(self):\n",
    "        self._information_crawler()\n",
    "    \n",
    "    \n",
    "    def _information_crawler(self):\n",
    "        self._read_url()\n",
    "        self._information_crawling()\n",
    "    \n",
    "    \n",
    "    def _read_url(self):\n",
    "        def _check_information_file_validation(_info):\n",
    "            if len(_info) == 0:\n",
    "                print(\"URL information file is empty. Please check.\")\n",
    "                sys.exit(0)\n",
    "            _valid_count = 0\n",
    "            for url in _info:\n",
    "                value = url.strip().split(\"@\")[1]\n",
    "                if len(value) != 0:\n",
    "                    _valid_count += 1\n",
    "            if _valid_count == 0 :\n",
    "                print(\"There is no url in the file. Please check.\")\n",
    "                sys.exit(0)\n",
    "         \n",
    "        try:\n",
    "            _url_file =  open('./url_information.txt', 'rt', encoding='utf8')\n",
    "        except FileNotFoundError as e:\n",
    "            print(\"The file containing url information doesn't exist in the same path. Please check.\")\n",
    "            sys.exit(0)\n",
    "        _url_information = _url_file.readlines()\n",
    "        _check_information_file_validation(_url_information)\n",
    "        self._set_url_dict(_url_information)\n",
    "        \n",
    "        \n",
    "    def _information_crawling(self):\n",
    "        def _filter_none_information(instruction, soup):\n",
    "            try:\n",
    "                exec(instruction)\n",
    "            except Exception:\n",
    "                raise AttributeError\n",
    "                    \n",
    "        _crawled_value_dict = {}    \n",
    "        url_dict = self._url_dict   \n",
    "        for key, value in url_dict.items():\n",
    "            url = url_dict[key]\n",
    "            html = requests.get(url)\n",
    "            plain_text = html.text   \n",
    "            soup = bs(plain_text, 'lxml')\n",
    "            self._set__soup_inst_dict()\n",
    "            area = soup.find('span', {'class': 'btn_select'}).find('em').text\n",
    "            for name, inst in self._soup_inst_dict.items():\n",
    "                try:\n",
    "                    _filter_none_information(inst, soup)\n",
    "                except AttributeError as e:\n",
    "                    print(f\"{name} information of {area} is none.\")\n",
    "                    _crawled_value_dict[name] = None\n",
    "                else:\n",
    "                    _crawled_value_dict[name] = eval(name) \n",
    "            _crawled_value_dict[\"uv_num\"], _crawled_value_dict[\"uv_non_num_content\"] = \\\n",
    "                                                    self._get_uv_information(area, soup)\n",
    "            for key, value in self._get_dust_and_ozone_information(area, soup).items():\n",
    "                _crawled_value_dict[key] = value\n",
    "            self._data_dict[area] = _crawled_value_dict\n",
    "            \n",
    "            \n",
    "\n",
    "    def _get_uv_information(self, region, soup):\n",
    "        try:\n",
    "            _uv_content = soup.find('span', {'class': 'indicator'}).find('span').text\n",
    "        except AttributeError:\n",
    "            print(f\"UV information of {region} is none.\")\n",
    "            return (None, None)\n",
    "        try:\n",
    "            _uv_num = soup.find('span', {'class': 'indicator'}).find('span', {'class': \"num\"}).text\n",
    "        except AttributeError:\n",
    "            print(f\"UV number information of {region} is none.\")\n",
    "            return (None, None)\n",
    "        _uv_non_num_content_point = _uv_content.find(_uv_num)+len(_uv_num)\n",
    "        _uv_non_num_content = _uv_content[_uv_non_num_content_point: ]\n",
    "        return (_uv_num, _uv_non_num_content)\n",
    "    \n",
    "    \n",
    "    def _get_dust_and_ozone_information(self, region, soup):\n",
    "        _fine_dust = None\n",
    "        _fine_dust_num = None\n",
    "        _ultra_fine_dust = None\n",
    "        _ultra_fine_dust_num = None\n",
    "        _ozone = None\n",
    "        _ozone_num = None\n",
    "        _dust_list = []\n",
    "        _ozone_text = \"\"\n",
    "        for ele in soup.find_all('dd'):\n",
    "            if \"㎍/㎥\" in ele.text:\n",
    "                _dust_list.append(ele.text)\n",
    "            elif \"ppm\" in ele.text:\n",
    "                _ozone_text = ele.text\n",
    "        try:\n",
    "            if 'null' not in _dust_list[0].split(\"㎍/㎥\"):\n",
    "                _fine_dust = _dust_list[0].split(\"㎍/㎥\")[1]\n",
    "                _fine_dust_num = _dust_list[0].split(\"㎍/㎥\")[0]\n",
    "            else:\n",
    "                print(f\"Some fine dust information of {region} are missing.\")\n",
    "        except IndexError:\n",
    "            print(f\"All fine dust information of {region} are none.\")\n",
    "           \n",
    "        try:\n",
    "            if 'null' not in _dust_list[1].split(\"㎍/㎥\"):\n",
    "                _ultra_fine_dust = _dust_list[1].split(\"㎍/㎥\")[1]\n",
    "                _ultra_fine_dust_num = _dust_list[1].split(\"㎍/㎥\")[0]\n",
    "            else:\n",
    "                print(f\"Some ultra fine dust information of {region} are missing.\")\n",
    "        except IndexError:\n",
    "            print(f\"Ultra fine dust information of {region} are none.\")\n",
    "            \n",
    "        if _ozone_text is not '':\n",
    "            _ozone = _ozone_text.split(\"ppm\")[1]\n",
    "            _ozone_num = _ozone_text.split(\"ppm\")[0]\n",
    "        else:\n",
    "            print(f\"Ozone information of {region} is none.\")\n",
    "        \n",
    "        _value_dict = {}\n",
    "        _value_dict['fine_dust'] = _fine_dust \n",
    "        _value_dict['fine_dust_num'] = _fine_dust_num \n",
    "        _value_dict['ultra_fine_dust'] = _ultra_fine_dust \n",
    "        _value_dict['ultra_fine_dust_num'] = _ultra_fine_dust_num \n",
    "        _value_dict['ozone'] = _ozone \n",
    "        _value_dict['ozone_num'] = _ozone_num\n",
    "        return _value_dict\n",
    "            \n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    weather_info = WeatherInformationCralwer()\n",
    "    weather_info.main()\n",
    "    print(weather_info.data_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
